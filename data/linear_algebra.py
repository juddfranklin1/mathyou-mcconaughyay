"""Data for linear algebra concepts."""

LINEAR_ALGEBRA_CONCEPTS = {
    "Vector": {
        "formula": r"\vec{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}",
        "explanation": "<p>Notation: $\\vec{v}$ denotes a vector, typically written as a column of numbers. This notation is used to distinguish vectors from scalars and matrices, and to emphasize their direction and magnitude in space. Vectors are fundamental objects in linear algebra, representing points, directions, or quantities with both magnitude and direction.</p>",
        "study_plan": "<h3>ðŸ’¡ Study Plan for Understanding Vectors</h3>\n\n<h4>Day 1: Vector Basics</h4>\n<ul>\n<li>Draw vectors on paper, starting with 2D</li>\n<li>Practice writing vectors in different notations</li>\n<li>Understand vector components and their meaning</li>\n</ul>\n\n<h4>Day 2: Vector Operations</h4>\n<ul>\n<li>Add and subtract vectors graphically</li>\n<li>Multiply vectors by scalars</li>\n<li>Find vector magnitudes</li>\n</ul>\n\n<h4>Day 3: Real Applications</h4>\n<ul>\n<li>Model real-world quantities as vectors (velocity, force)</li>\n<li>Solve simple physics problems using vectors</li>\n<li>Create vector graphics on a computer</li>\n</ul>"
    },
    "Matrix Multiplication": {
        "formula": r"(AB)_{ij} = \sum_{k=1}^n a_{ik}b_{kj}",
        "explanation": "<p>Think of matrix multiplication as a recipe for combining transformations. When you multiply matrices $A$ and $B$, you're saying 'do transformation $B$ first, then apply transformation $A$'. It's like following a dance routine: first step this way, then that way!</p>\n\n<p>The notation $(AB)_{ij}$ tells us how to find each element in the result. It's like filling out a puzzle where each piece (entry) needs its own calculation. The formula $\\sum_{k=1}^n a_{ik}b_{kj}$ might look scary, but it's just saying 'take matching pairs from $A$'s row and $B$'s column, multiply them, and add everything up'.</p>\n\n<p>Why can't we just multiply the matching elements like we do with numbers? Because matrices represent transformations, and when you do one transformation after another, you need to account for how they interact. It's like following a recipe - you can't just mix all ingredients at once; the order matters!</p>",
        "study_plan": "<h3>ðŸ’¡ Study Plan for Matrix Multiplication</h3>\n\n<h4>Day 1: Basic Multiplication</h4>\n<ul>\n<li>Start with 2Ã—2 matrices using simple numbers</li>\n<li>Practice the 'row times column' pattern</li>\n<li>Draw diagrams to visualize the process</li>\n</ul>\n\n<h4>Day 2: Properties & Patterns</h4>\n<ul>\n<li>Discover why AB â‰  BA (use examples!)</li>\n<li>Find patterns with identity matrices</li>\n<li>Practice with zero matrices</li>\n</ul>\n\n<h4>Day 3: Real Applications</h4>\n<ul>\n<li>Transform shapes using matrix multiplication</li>\n<li>Combine rotations and scaling</li>\n<li>Solve real-world problems</li>\n</ul>\n\n<h4>Day 4: Advanced Practice</h4>\n<ul>\n<li>Work with larger matrices</li>\n<li>Use technology to check your work</li>\n<li>Create your own matrix multiplication problems</li>\n</ul>"
    },
    "Determinant": {
        "formula": r"\det(A) = \sum_{j=1}^n (-1)^{j+1} a_{1j} M_{1j}",
        "explanation": "<p>Think of a determinant as your matrix's 'impact score' â€“ it tells you how much your matrix transforms the world around it! Imagine you have a rubber stamp in the shape of a perfect square. When you apply a matrix transformation, it's like squishing, stretching, or rotating that stamp. The determinant tells you exactly how much bigger or smaller your stamp's imprint becomes.</p>\n\n<p>Let's break it down with something we all love: pizza! Start with a square pizza box. If you have a matrix with determinant 2, it's like magically transforming your pizza box to hold twice as much pizza (yum!). If the determinant is Â½, your pizza box shrinks to half its size (maybe it's a personal pizza now). A negative determinant, like -1, means your box gets flipped over â€“ same size, but upside down (careful with the toppings!). And if you get a determinant of zero? That's like squishing your pizza box so flat that it can't hold any pizza at all!</p>\n\n<p>Calculating a determinant is easier than you might think. For a 2Ã—2 matrix $\\begin{pmatrix}a & b \\\\ c & d\\end{pmatrix}$, just remember this trick: multiply diagonally and subtract, like a tiny crossword puzzle! Go down-right ($a \\times d$) and subtract down-left ($b \\times c$). Let's try a real example: if you're scaling a photo using the matrix $\\begin{pmatrix}3 & 1 \\\\ 2 & 4\\end{pmatrix}$, the determinant is $(3 \\times 4) - (1 \\times 2) = 12 - 2 = 10$. This means your photo will become 10 times larger in area â€“ hope you have enough wall space!</p>\n\n<p>Why should you care about determinants? They're like a safety check for your mathematical operations. In computer games, when characters move around, the game needs to make sure they can move back to their original position â€“ that's where non-zero determinants come in handy. Your smartphone's screen rotation works because of matrices with determinants of 1 or -1, perfectly preserving the size of everything you see. Even your favorite photo filters use matrices with carefully chosen determinants to make sure your selfies don't get warped in weird ways!</p>",
        "core_idea": "The determinant is a single number that tells you a ton about a square matrix. For a 2x2 matrix $\\begin{bmatrix} a & b \\ c & d \\end{bmatrix}$, the determinant is $ad-bc$. Geometrically, its absolute value tells you the 'scaling factor' of the transformation. If you transform a square of area 1 with the matrix, the new shape (a parallelogram) will have an area equal to $|det(A)|$. If the determinant is 0, the matrix squishes space into a lower dimension (like a line or a point), and it's not invertible!",
        "real_world_application": "In computer graphics, if you're about to apply a transformation matrix to a million-point 3D model, you might calculate the determinant first. If it's zero, you know the transformation will collapse the model flat, which might be a bug. It's a quick check to see if your transformation is 'valid' and reversible.",
        "mathematical_demonstration": "Let's find the determinant of $A = \\begin{bmatrix} 4 & 2 \\ 1 & 3 \\end{bmatrix}$.<br>1. Identify a, b, c, and d. Here, $a=4, b=2, c=1, d=3$.<br>2. Apply the formula $det(A) = ad - bc$.<br>3. Substitute: $det(A) = (4)(3) - (2)(1) = 12 - 2 = 10$.<br>This tells us two things: 1) The matrix is invertible (since the determinant isn't 0). 2) The transformation scales the area of any shape by a factor of 10.",
        "study_plan": "<h3>ðŸ’¡ Study Plan for Mastering Determinants</h3>\n\n<h4>Day 1-2: Start Small</h4>\n<ul>\n<li>Practice with 2Ã—2 matrices only</li>\n<li>Draw little boxes and actually track the diagonals with different colored pens</li>\n<li>Try simple numbers first: $\\begin{pmatrix}2 & 0 \\\\ 0 & 3\\end{pmatrix}$ (This scales width by 2 and height by 3)</li>\n</ul>\n\n<h4>Day 3-4: Get Real</h4>\n<ul>\n<li>Find determinants of real-world transformations</li>\n<li>Rotate a square by 90Â° using $\\begin{pmatrix}0 & -1 \\\\ 1 & 0\\end{pmatrix}$ (determinant = 1)</li>\n<li>Scale a rectangle using different numbers</li>\n</ul>\n\n<h4>Day 5-6: Explore Patterns</h4>\n<ul>\n<li>What happens when you multiply matrices?</li>\n<li>Why do determinants multiply too?</li>\n<li>Play with negative numbers and see what they do</li>\n</ul>\n\n<h4>Day 7: Connect the Dots</h4>\n<ul>\n<li>Try simple linear equations and see why zero determinants are trouble</li>\n<li>Practice with your phone's rotation - it's matrices in action!</li>\n<li>Create your own examples using things around you</li>\n</ul>\n\n<p><em>Remember: Every time you calculate a determinant, you're measuring how much your matrix changes the world. It's not just numbers â€“ it's a story about transformation! ðŸŽ¯</em></p>"
    },
    "Eigenvalue": {
        "formula": r"A\vec{x} = \lambda\vec{x}",
        "explanation": "Notation: $A\\vec{x} = \\lambda\\vec{x}$ means that when matrix $A$ acts on vector $\\vec{x}$, the result is a scaled version of $\\vec{x}$ by $\\lambda$. This notation is used to highlight special vectors (eigenvectors) that only get stretched or shrunk, not rotated, by the transformation."
    },
    "Rank": {
        "formula": r"\text{rank}(A)",
        "explanation": "<p>Imagine your matrix is a super-intelligent coffee machine. You can put in all sorts of fancy coffee bean combinations (your input vectors), but the machine can only produce a few distinct types of coffee (your output vectors). The 'rank' is the number of unique, truly different coffees it can make. If it can make an espresso, a latte, and a flat white that are all genuinely different, its rank is 3. But if its 'cappuccino' is just a latte with some chocolate powder, that's not a new 'independent' coffee, so it doesn't increase the rank!</p>\n\n<p>In math terms, the rank tells you the number of 'dimensions' your matrix's output lives in. If you have a 3D vector and you apply a matrix with rank 2, it's like squishing your 3D world onto a flat 2D plane. All the original depth is gone! A matrix with rank 1 would squish everything onto a single line. A rank-0 matrix? That's a mathematical black hole that squishes everything down to a single point (the origin)!</p>\n\n<p>So, why is this 'coffee-making ability' so important? The rank tells you if your system of equations has a solution, if a transformation is reversible, and how much information is preserved. It's the matrix's true 'power' level â€“ not its size, but what it can actually do!</p>",
        "core_idea": "The rank of a matrix is the number of linearly independent columns (or rows). It tells you the dimension of the output space of the transformation. If an $m \\times n$ matrix has rank $r$, it means that no matter what vector you multiply it by, the result will always lie within some $r$-dimensional subspace. A matrix is 'full rank' if its rank is the largest it can be, which is $\\min(m, n)$.",
        "real_world_application": "In data science, you might have a dataset with hundreds of features (columns). Calculating the rank can tell you if some features are redundant (e.g., if 'height in feet' and 'height in inches' are both included). A low rank suggests the data can be simplified or compressed without losing much information, which is the core idea behind techniques like Principal Component Analysis (PCA).",
        "mathematical_demonstration": "Let's find the rank of $A = \\begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \\end{bmatrix}$.<br>1. Look at the columns: $\\vec{c_1} = \\begin{bmatrix} 1 \\ 2 \\end{bmatrix}$, $\\vec{c_2} = \\begin{bmatrix} 2 \\ 4 \\end{bmatrix}$, $\\vec{c_3} = \\begin{bmatrix} 3 \\ 6 \\end{bmatrix}$.<br>2. Check for linear independence. Is any column a multiple of another? <br>Yes! $\\vec{c_2} = 2 \\times \\vec{c_1}$ and $\\vec{c_3} = 3 \\times \\vec{c_1}$.<br>3. All the columns are just scaled versions of the first column. This means there is only one 'independent' direction. <br>Therefore, the rank of A is 1. Even though the matrix operates in 2D space and has 3 columns, its output is always stuck on the line defined by the vector $\\begin{bmatrix} 1 \\ 2 \\end{bmatrix}$.",
        "study_plan": "<h3>ðŸ’¡ Study Plan for Understanding Rank</h3>\n\n<h4>Day 1: The Basics</h4>\n<ul>\n<li>Take simple matrices and identify the column vectors.</li>\n<li>Practice checking if one column is a multiple of another in 2x2 or 2x3 matrices.</li>\n<li>Think about what 'independent' means. (e.g., North-South is independent of East-West).</li>\n</ul>\n\n<h4>Day 2: Row Echelon Form</h4>\n<ul>\n<li>Learn to use Gaussian elimination to get a matrix into row echelon form.</li>\n<li>Count the number of non-zero rows. That's the rank! This is the most reliable way to calculate it.</li>\n<li>Practice with 3x3 matrices.</li>\n</ul>\n\n<h4>Day 3: Connect to Geometry</h4>\n<ul>\n<li>Take a rank 1 matrix and multiply it by a few random vectors. Plot the results. You'll see they all land on a line!</li>\n<li>Do the same for a rank 2 matrix. The results will land on a plane.</li>\n<li>What does a rank 0 matrix do? (Hint: It's not very exciting!)</li>\n</ul>\n\n<h4>Day 4: Connect to Systems of Equations</h4>\n<ul>\n<li>Create a system of equations $A\\vec{x}=\\vec{b}$ where A is low-rank. See what happens when you try to solve it.</li>\n<li>Understand why a square matrix must be full-rank to be invertible.</li>\n</ul>\n\n<p><em>Remember: Rank isn't about the size of the matrix, but its 'effectiveness'. It's the true measure of a matrix's power to transform space! ðŸš€</em></p>"
    },
    "Identity Matrix": {
        "formula": r"I_n = \begin{bmatrix} 1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1 \end{bmatrix}",
        "explanation": "Notation: $I_n$ denotes the $n \\times n$ identity matrix, which has ones on the diagonal and zeros elsewhere. This notation is used because the identity matrix acts as the multiplicative identity in matrix algebra: $AI_n = I_nA = A$ for any compatible matrix $A$."
    },
    "Transpose": {
        "formula": r"A^T_{ij} = A_{ji}",
        "explanation": "Notation: $A^T$ denotes the transpose of matrix $A$, which flips rows and columns. This notation is used to indicate the operation of switching the orientation of a matrix, which is important in many algebraic manipulations and proofs."
    },
    "Inverse": {
        "formula": r"AA^{-1} = A^{-1}A = I",
        "explanation": "Notation: $A^{-1}$ denotes the inverse of matrix $A$, which satisfies $AA^{-1} = I$. This notation is used because the inverse matrix undoes the transformation represented by $A$, analogous to division for numbers."
    },
    "Dot Product": {
        "formula": r"\vec{a} \cdot \vec{b} = \sum_{i=1}^n a_i b_i",
        "explanation": "Notation: $\\vec{a} \\cdot \\vec{b}$ denotes the dot product of two vectors, calculated as the sum of the products of their components. This notation is used because the dot product measures the similarity or projection of one vector onto another, and is fundamental in geometry and physics."
    },
    "Cross Product": {
        "formula": r"\vec{a} \times \vec{b} = \begin{bmatrix} a_2b_3 - a_3b_2 \\ a_3b_1 - a_1b_3 \\ a_1b_2 - a_2b_1 \end{bmatrix}",
        "explanation": "Notation: $\\vec{a} \\times \\vec{b}$ denotes the cross product of two 3-dimensional vectors, resulting in a new vector perpendicular to both. This notation is used because the cross product encodes the area and orientation of the parallelogram formed by the vectors."
    },
    "Linear Transformation": {
        "formula": r"T(\vec{x}) = A\vec{x}",
        "explanation": "Notation: $T(\\vec{x}) = A\\vec{x}$ expresses a linear transformation $T$ applied to vector $\\vec{x}$ as multiplication by matrix $A$. This notation is used because it connects abstract transformations to concrete matrix operations, making computations and theory more accessible."
    },
    "System of Linear Equations": {
        "formula": r"A\vec{x} = \vec{b}",
        "explanation": "Notation: $A\\vec{x} = \\vec{b}$ represents a system of linear equations, where $A$ is the coefficient matrix, $\\vec{x}$ is the vector of unknowns, and $\\vec{b}$ is the vector of constants. This notation is used because it compactly expresses multiple equations and enables solution methods using matrix algebra."
    }
}